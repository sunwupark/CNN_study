{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### LearningRateScheduler callback 객체에 입력될 scheduler 함수 선언. \n* epoch 증가 시 마다 LR이 exponentially decay 되도록 설정. ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\ndef scheduler_exp(epoch, lr):\n    if epoch < 1:\n        return lr\n    else:\n        return lr * np.exp(-1.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.exp(-2.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LearningRateScheduler 로 Learning Rate를 epochs 시마다 변경하기\n* LearningRateScheduler 객체 생성 시 인자로 scheduler 함수 입력하여 생성\n* model.fit() callbacks인자로 LearningRateScheduler 객체 입력","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\n\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\n\ndef scheduler_exp(epoch, lr):\n    if epoch < 1:\n        return lr\n    else:\n        return lr * np.exp(-1.0)\n    \n# 테스트용 임시 모델을 생성. \nmodel = Sequential([Dense(10)])\nmodel.compile(optimizer=SGD(), loss='mse')\nprint('최초 learning rated:', round(model.optimizer.lr.numpy(), 5))\n\n# LearningRateScheduler 객체에 인자로 scheduler_exp 함수 등록 \nlr_scheduler = LearningRateScheduler(scheduler_exp, verbose=1)\n\nhistory = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n                    epochs=15, callbacks=[lr_scheduler], verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### scheduler_exp 함수 결과를 시각화","metadata":{}},{"cell_type":"code","source":"def scheduler_exp(epoch):\n    initial_lr = 0.01\n    if epoch < 1:\n        return initial_lr\n    else:\n        return initial_lr * np.exp(-1.0)**epoch\n\n    \nmodel = Sequential([Dense(10)])\nmodel.compile(optimizer=SGD(), loss='mse')\nprint('최초 learning rated:', round(model.optimizer.lr.numpy(), 5))\n\n# LearningRateScheduler 객체에 인자로 scheduler_exp 함수 등록 \nlr_scheduler = LearningRateScheduler(scheduler_exp, verbose=1)\nhistory = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n                    epochs=15, callbacks=[lr_scheduler], verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs_list = range(30)\nlr_list = [ scheduler_exp(epoch) for epoch in epochs_list]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef plot_scheduler(epochs_list, lr_list, title=None):\n    plt.figure(figsize=(6,4))\n    plt.plot(epochs_list, lr_list)\n    plt.xlabel('epochs')\n    plt.ylabel('learning rate')\n    plt.title(title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_scheduler(epochs_list, lr_list, 'exponentially decay')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 또 다른 함수(step_decay)로 LearningRateScheduler 객체 생성하여 LR 조절. ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ndef step_decay(epoch):\n    initial_lr = 0.1\n    drop = 0.5\n    epochs_drop = 5.0\n    lr = initial_lr * (drop ** np.floor((epoch)/epochs_drop))\n    print('epoch:',epoch,'lr:', lr)\n    return lr\n\nlr_list = [step_decay(epoch) for epoch  in epochs_list]\nplot_scheduler(epochs_list, lr_list, title='Step Decay')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential([Dense(10)])\nmodel.compile(optimizer=SGD(), loss='mse')\nprint('최초 learning rated:', round(model.optimizer.lr.numpy(), 5))\n\nlr_scheduler = LearningRateScheduler(step_decay, verbose=1)\nhistory = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n                    epochs=15, callbacks=[lr_scheduler], verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cosine Decay 적용하기\n* Tensorflow 2.x은 Cosine Decay를 tf.keras.experimental.CosineDecay로 제공\n* tf.keras.experimental.CosineDecay는 callback이 아니라 optimizer의 LearningRateSchedule 이며, model.compile()의 optimizer 인자의 learning_rate로 값 입력 부여\n* initial_learning_rate는 최초 LR\n* decay_steps는 감소 적용할 steps \n* alpha는 최소 LR\n","metadata":{}},{"cell_type":"code","source":"cos_decay = tf.keras.experimental.CosineDecay(initial_learning_rate=1e-2, decay_steps=30, alpha=1e-2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(cos_decay)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef plot_scheduler(epochs_list, lr_list, title=None):\n    plt.figure(figsize=(6,4))\n    plt.plot(epochs_list, lr_list)\n    plt.xlabel('epochs')\n    plt.ylabel('learning rate')\n    plt.title(title)\n\ncos_decay = tf.keras.experimental.CosineDecay(initial_learning_rate=1e-2, decay_steps=30, alpha=1e-2)\n\nsteps_list = range(0, 30)\nlr_list = cos_decay(steps_list)\n\nplot_scheduler(steps_list, lr_list, 'Cosine Decay')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### tf.keras.experimental.CosineDecay는 optimizer의 learning_rate 인자로 입력 되어야 함\n* Callback이 아니므로 epochs시마다 변경되는 learning rate값을 알 수가 없음","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n# CosineDecay 객체는 optimizer의 learning_rate 인자로 입력되어야 함. \nmodel.compile(tf.keras.optimizers.Adam(learning_rate=cos_decay), loss='mse')\n\nhistory = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n                    epochs=15, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decay_steps = 30\nalpha = 0.0\ninitial_learning_rate = 0.01\n\ndef decayed_learning_rate(step):\n    step = min(step, decay_steps)\n    cosine_decay = 0.5 * (1 + np.cos(np.pi * step / decay_steps))\n    decayed = (1 - alpha) * cosine_decay + alpha\n    return initial_learning_rate * decayed\n\nsteps_list = range(0, 30)\nlr_list = [decayed_learning_rate(step) for step in steps_list]\n\nplot_scheduler(steps_list, lr_list, 'Cosine Decay')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cosine Decay Restart(Cosine Annealing) 적용하기\n* tf.keras.experimental.CosineDecayRestarts 객체를 이용하여 적용\n* CosineDecay 객체와 마찬가지로 Callback이 아니므로 optimizer의 learning_rate인자로 입력 되어야 함. \n* initial_learning_rate는 최초 LR, first_decay_steps는 최초 decay step 수\n* t_mul는 전체 steps수를 감안해서 얼마나 cosine annealing을 반복할 지 결정하는 계수. 1이면 전체 steps/first_decay_steps, 2이면 분모를 이전 값 대비 2배로 증가 시킴\n* m_mul은 warm restart시 수행시마다 적용될 초기 learning rate\n* alpha는 최소 learning rate로 설정 계수로 최소 lr은 initial learning rate * alpha로 설정됨. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.experimental import CosineDecayRestarts\n\ncos_decay_restarts = CosineDecayRestarts(initial_learning_rate=0.01, first_decay_steps=10, t_mul=1, m_mul=0.9, alpha=0)\nsteps_list = range(0, 120)\nlr_list = cos_decay_restarts(steps_list)\n\nplot_scheduler(steps_list, lr_list, 'Cosine Decay Restarts')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n# CosineDecay 객체는 optimizer의 learning_rate 인자로 입력되어야 함. \nmodel.compile(tf.keras.optimizers.Adam(learning_rate=cos_decay_restarts), loss='mse')\n\nhistory = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n                    epochs=15, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Ramp Up and Step Down Decay\n* Ramp up으로 warm up 수행 후 Step 형식으로 Decay 하는 방식. \n* Kaggle의 Chris Deotte 가 제안하여 사용됨. ","metadata":{}},{"cell_type":"code","source":"LR_START = 1e-5\nLR_MAX = 1e-2\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 10\nLR_STEP_DECAY = 0.75\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = ((LR_MAX - LR_START) / LR_RAMPUP_EPOCHS) * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//2)\n    #print('epoch:', epoch, 'lr:', lr)\n    return lr\n\nlr_list = [lrfn(epoch) for epoch  in epochs_list]\n\nplot_scheduler(epochs_list, lr_list, title='WarmUp')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 아래와 같이 내포 함수로 생성하는 것을 권장. ","metadata":{}},{"cell_type":"code","source":"def lrfn(epoch):\n    LR_START = 1e-5\n    LR_MAX = 1e-2\n    LR_RAMPUP_EPOCHS = 3\n    LR_SUSTAIN_EPOCHS = 3\n    LR_STEP_DECAY = 0.75\n    \n    def calc_fn(epoch):\n        if epoch < LR_RAMPUP_EPOCHS:\n            lr = ((LR_MAX - LR_START) / LR_RAMPUP_EPOCHS) * epoch + LR_START\n        elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n            lr = LR_MAX\n        else:\n            lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//2)\n        \n        print('epoch:', epoch, 'lr:', lr)\n        \n        return lr\n    \n    # 반드시 내포 함수인 calc_fn(epoch)를 호출해야함. \n    return calc_fn(epoch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\n\nmodel = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\nmodel.compile(tf.keras.optimizers.SGD(), loss='mse')\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lrfn)\nhistory = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n                    epochs=30, callbacks=[lr_scheduler], verbose=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cosine Decay를 Callback 으로 구현. ","metadata":{}},{"cell_type":"code","source":"from collections.abc import Iterable\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.framework import constant_op\nimport math\n\n\nclass CosineDecayCB(tf.keras.callbacks.Callback):\n    def __init__(self, initial_learning_rate, decay_steps):\n        super(CosineDecayCB, self).__init__()\n        self.initial_learning_rate = initial_learning_rate\n        self.decay_steps = decay_steps\n        self.batch_step = 0\n        self.calc_lr = 0.0\n\n    def on_train_batch_begin(self, step, logs=None):\n        if not hasattr(self.model.optimizer, \"lr\"):\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\n        # optimizer의 현재 learning rate를 가져옴. \n        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n        # schedule 함수에 batch_step과 lr을 입력하여 step에 따른 lr 계산. \n        scheduled_lr = self.schedule(self.batch_step, lr)\n        # 계산된 scheduled_lr을 optimizer의 lr로 입력. \n        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n        self.batch_step += 1\n        self.calc_lr = scheduled_lr\n        \n    def on_epoch_end(self, epoch, logs=None):\n        print('learning rate:', self.calc_lr)\n        \n    # cosine decay learning rate를 step 별로 계산. \n    def schedule(self, step, lr):\n        def decayed_learning_rate(step, lr):\n            step = min(step, self.decay_steps)\n            cosine_decay = 0.5 * (1 + np.cos(np.pi * step / self.decay_steps))\n            decayed = (1 - alpha) * cosine_decay + alpha\n            return lr * decayed\n        \n        return decayed_learning_rate(step, lr)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\nmodel.compile(tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse')\n\ncosine_scheduler = CosineDecayCB(0.01, 100)\n#lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n#lr_monitor = LearningRateMonitor()\nhistory = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n                    epochs=30, callbacks=[cosine_scheduler], verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cosine Decay Restart 를 Callback으로 구현. \n* https://gist.github.com/jeremyjordan/5a222e04bb78c242f5763ad40626c452","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\nclass LearningRateMonitor(Callback):\n    # start of training\n    def on_train_begin(self, logs={}):\n        self.lrates = list()\n\n    # end of each training epoch\n    def on_epoch_end(self, epoch, logs={}):\n        # get and store the learning rate\n        optimizer = self.model.optimizer\n        lrate = float(K.get_value(self.model.optimizer.lr))\n        print('learning rate:', lrate)\n        self.lrates.append(lrate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://gist.github.com/jeremyjordan/5a222e04bb78c242f5763ad40626c452\n\nfrom collections.abc import Iterable\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.framework import constant_op\nimport math\n\n\nclass CosineDecayRestarts(tf.keras.callbacks.Callback):\n    def __init__(self, initial_learning_rate, first_decay_steps, alpha=0.0, t_mul=2.0, m_mul=1.0):\n        super(CosineDecayRestarts, self).__init__()\n        self.initial_learning_rate = initial_learning_rate\n        self.first_decay_steps = first_decay_steps\n        self.alpha = alpha\n        self.t_mul = t_mul\n        self.m_mul = m_mul\n        self.batch_step = 0\n\n    def on_train_batch_begin(self, step, logs=None):\n        if not hasattr(self.model.optimizer, \"lr\"):\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\n        # Get the current learning rate from model's optimizer.\n        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n        # Call schedule function to get the scheduled learning rate.\n        scheduled_lr = self.schedule(self.batch_step, lr)\n        # Set the value back to the optimizer before this epoch starts\n        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n        self.batch_step += 1\n\n    def schedule(self, step, lr):\n        def compute_step(completed_fraction, geometric=False):\n            \"\"\"Helper for `cond` operation.\"\"\"\n            if geometric:\n                i_restart = math_ops.floor(\n                  math_ops.log(1.0 - completed_fraction * (1.0 - self.t_mul)) /\n                  math_ops.log(self.t_mul))\n\n                sum_r = (1.0 - self.t_mul**i_restart) / (1.0 - self.t_mul)\n                completed_fraction = (completed_fraction - sum_r) / self.t_mul**i_restart\n\n            else:\n                i_restart = math_ops.floor(completed_fraction)\n                completed_fraction -= i_restart\n\n            return i_restart, completed_fraction\n\n        completed_fraction = step / self.first_decay_steps\n\n        i_restart, completed_fraction = control_flow_ops.cond(\n          math_ops.equal(self.t_mul, 1.0),\n          lambda: compute_step(completed_fraction, geometric=False),\n          lambda: compute_step(completed_fraction, geometric=True))\n\n        m_fac = self.m_mul**i_restart\n        cosine_decayed = 0.5 * m_fac * (1.0 + math_ops.cos(\n          constant_op.constant(math.pi) * completed_fraction))\n        decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n\n        return math_ops.multiply(self.initial_learning_rate, decayed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\nmodel.compile(tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse')\n\ncosine_scheduler = CosineDecayRestarts(0.01, 100)\nlr_monitor = LearningRateMonitor()\nhistory = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n                    epochs=15, callbacks=[cosine_scheduler, lr_monitor], verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}