{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Stanford Dog Breed 데이터 세트를 아래 URL에서 직접 Download 및 압축 해제\n* Kaggle의 Dataset으로 Object Storage 연결 시 이미지를 한장 씩 읽는 데 많은 시간이 소요되어 모델 학습에 시간이 더 걸림. \n* Local Disk에 바로 이미지를 다운로드/압축 해제 후 모델에서 이를 이용할 수 있도록 함. ","metadata":{}},{"cell_type":"code","source":"# stanford dog breed 데이터 세트 다운로드 \n!wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n# 현재 디렉토리인 /kaggle/working에 바로 압축 해제 \n!ls; tar -xvf images.tar","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 현재 디렉토리의 내용 확인. \n!ls; pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd /kaggle/working/Images;ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/working/Images'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 이미지 파일들의 디렉토리와 파일명을 기반으로 메타 정보인 이미지 절대경로, 레이블을 DataFrame으로 생성\n* /kaggle/working/Images 디렉토리 밑에 Dog breed 서브 디렉토리와 이미지 파일로 구성 되어 있음. \n* 레이블 값은 이미지 파일의 절대경로에서 이미지 파일 바로 위에 있는 서브 디렉토리를 가공하여 생성. ","metadata":{}},{"cell_type":"code","source":"start_pos = '/kaggle/working/Images/n02109961-Eskimo_dog/n02109961_12338.jpg'.find('/', 20)\nend_pos = '/kaggle/working/Images/n02109961-Eskimo_dog/n02109961_12338.jpg'.rfind('/')\n\nimsi_breed = '/kaggle/working/Images/n02109961-Eskimo_dog/n02109961_12338.jpg'[start_pos+1:end_pos]\nprint(start_pos, end_pos, imsi_breed)\nimsi_breed[imsi_breed.find('-')+1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_DIR = '/kaggle/working/Images' \n\ndef make_dogbreed_dataframe(image_dir=IMAGE_DIR):\n    paths = []\n    label_gubuns = []\n    for dirname, _, filenames in os.walk(image_dir):\n        for filename in filenames:\n            # 이미지 파일이 아닌 파일도 해당 디렉토리에 있음.\n            if '.jpg' in filename:\n                # 파일의 절대 경로를 file_path 변수에 할당. \n                file_path = dirname+'/'+ filename\n                paths.append(file_path)\n                # 이미지 파일의 절대 경로에서 레이블명 생성을 위한 1차 추출. '/'로 분할하여 파일 바로 위 서브디렉토리 이름 가져옴.  \n                start_pos = file_path.find('/', 20)\n                end_pos = file_path.rfind('/')\n                imsi_breed = file_path[start_pos+1:end_pos]\n                # 1차 추출된 데이터를 기반으로 '-' 이후 데이터가 레이블 값임. \n                breed = imsi_breed[imsi_breed.find('-')+1:]\n                #print(start_pos, end_pos, imsi_breed)\n                label_gubuns.append(breed)\n\n    data_df = pd.DataFrame({'path':paths, 'label':label_gubuns})\n    return data_df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 200)\ndata_df = make_dogbreed_dataframe()\nprint('data_df shape:', data_df.shape)\ndata_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dog Breed의 개별 분포도 확인. ","metadata":{}},{"cell_type":"code","source":"print(data_df.shape)\n# breed 별 건수 확인\ndata_df['label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 막대 그래프 형태로 breed별 건수 확인 \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nplt.figure(figsize=(26, 4))\n\nsns.countplot(data=data_df, x='label')\nplt.xticks(rotation=90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dog Breed의 이미지 보기","metadata":{}},{"cell_type":"code","source":"import cv2\n\n# dog breed 별로 image를 보기 위한 utility 함수 생성.\ndef show_grid_images(image_path_list, ncols=8, title=None):\n    figure, axs = plt.subplots(figsize=(22, 4), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        image = cv2.cvtColor(cv2.imread(image_path_list[i]), cv2.COLOR_BGR2RGB)\n        axs[i].imshow(image)\n        axs[i].set_title(title)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breed_image_list_01 = data_df[data_df['label']=='Staffordshire_bullterrier']['path'].iloc[:6].tolist()\nbreed_image_list_02 = data_df[data_df['label']=='American_Staffordshire_terrier']['path'].iloc[:6].tolist()\n\nshow_grid_images(breed_image_list_01, ncols=6, title='Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_02, ncols=6, title='American_Staffordshire_terrier')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df['label'].value_counts().index.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breed_list = data_df['label'].value_counts().index.tolist()\n\nfor iter_cnt, breed in enumerate(breed_list):\n    breed_image_list = data_df[data_df['label']==breed]['path'].iloc[:6].tolist()\n    show_grid_images(breed_image_list, ncols=6, title=breed)\n    if iter_cnt == 4:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 이미지 사이즈를 224x224로 고정하고 Augmentation 적용한 이미지 살펴 보기","metadata":{}},{"cell_type":"code","source":"import albumentations as A\n\n# crop은 사용에 주의할것. 사람과 개가 같이 있으므로 center에 사람이 있을 경우 사람만 이미지가 잘릴수 있음. \nimsi_augmentor = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.ShiftScaleRotate(p=0.5),\n    #A.CenterCrop(height=200, width=200, p=0.5),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.5),\n    A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5)\n])\n\n# augmented 적용된 이미지를 보기 위해서 위에서 albumetation으로 image 변환 기법 적용된 transformer 입력\n# 이미지 사이즈를 224x224로 resize 적용. \ndef show_grid_images(image_path_list, augmentor=None, ncols=4, title=None):\n    figure, axs = plt.subplots(figsize=(22, 4), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        image = cv2.cvtColor(cv2.imread(image_path_list[i]), cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (224, 224))\n        if augmentor is not None:\n            image = augmentor(image=image)['image']\n        axs[i].imshow(image)\n        axs[i].axis('off')\n        axs[i].set_title(title) \n        \nbreed_image_list_01 = data_df[data_df['label']=='Staffordshire_bullterrier']['path'].iloc[:6].tolist()       \nshow_grid_images(breed_image_list_01, augmentor=None, ncols=6, title='orignal Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_01, augmentor=imsi_augmentor, ncols=6, title='augmented Staffordshire_bullterrier')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 전체 DataFrame을 학습과 테스트용 DataFrame으로 분리. 학습 DataFrame은 다시 학습과 검증용으로 분리 \n* train_test_split()을 이용하여 전체의 40%를 테스트 데이터로 할당. stratify인자로 breed label별로 균등하게 할당 설정. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(data_df, test_size=0.4, stratify=data_df['label'], random_state=2021)\nprint(train_df.shape, test_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df['label'].value_counts()/train_df.shape[0])\nprint(test_df['label'].value_counts()/test_df.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# DataFrame에서 numpy array로 변환. \ntrain_path = train_df['path'].values\ntrain_label = pd.get_dummies(train_df['label']).values\n# 학습 데이터를 다시 학습과 검증용으로 분할. \ntr_path, val_path, tr_label, val_label = train_test_split(train_path, train_label, stratify=train_label, test_size=0.2, random_state=0)\nprint('학습용 path shape:', tr_path.shape, '검증용 path shape:', val_path.shape, \n      '학습용 label shape:', tr_label.shape, '검증용 label shape:', val_label.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sequence 기반의 Dataset 생성. \n* 추후에 efficientb0에서 efficientb1으로 모델 변경하기 위해 image size를 인자 추가\n* Albumentations의 Crop등을 적용하기 위해 augmentation 적용 후 cv2.resize() 로직 변경.\n* augmentation은 기본적인 좌우반전(Horizontal) 먼저 적용. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import Sequence\nimport sklearn \nimport cv2\n\nBATCH_SIZE = 64\nIMAGE_SIZE = 224\n\n# 입력 인자 image_filenames, labels는 모두 numpy array로 들어옴. \nclass Breed_Dataset(Sequence):\n    def __init__(self, image_filenames, labels, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                 augmentor=None, shuffle=False, pre_func=None):\n        '''\n        파라미터 설명\n        image_filenames: opencv로 image를 로드할 파일의 절대 경로들\n        labels: 해당 image의 label들\n        batch_size: __getitem__(self, index) 호출 시 마다 가져올 데이터 batch 건수\n        augmentor: albumentations 객체\n        shuffle: 학습 데이터의 경우 epoch 종료시마다 데이터를 섞을지 여부\n        '''\n        # 객체 생성 인자로 들어온 값을 객체 내부 변수로 할당. \n        self.image_filenames = image_filenames\n        self.labels = labels\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.augmentor = augmentor\n        self.pre_func = pre_func\n        # train data의 경우 \n        self.shuffle = shuffle\n        if self.shuffle:\n            # 객체 생성시에 한번 데이터를 섞음. \n            #self.on_epoch_end()\n            pass\n    \n    # Sequence를 상속받은 Dataset은 batch_size 단위로 입력된 데이터를 처리함. \n    # __len__()은 전체 데이터 건수가 주어졌을 때 batch_size단위로 몇번 데이터를 반환하는지 나타남\n    def __len__(self):\n        # batch_size단위로 데이터를 몇번 가져와야하는지 계산하기 위해 전체 데이터 건수를 batch_size로 나누되, 정수로 정확히 나눠지지 않을 경우 1회를 더한다. \n        return int(np.ceil(len(self.labels) / self.batch_size))\n    \n    # batch_size 단위로 image_array, label_array 데이터를 가져와서 변환한 뒤 다시 반환함\n    # 인자로 몇번째 batch 인지를 나타내는 index를 입력하면 해당 순서에 해당하는 batch_size 만큼의 데이타를 가공하여 반환\n    # batch_size 갯수만큼 변환된 image_array와 label_array 반환. \n    def __getitem__(self, index):\n        # index는 몇번째 batch인지를 나타냄. \n        # batch_size만큼 순차적으로 데이터를 가져오려면 array에서 index*self.batch_size:(index+1)*self.batch_size 만큼의 연속 데이터를 가져오면 됨\n        image_name_batch = self.image_filenames[index*self.batch_size:(index+1)*self.batch_size]\n        if self.labels is not None:\n            label_batch = self.labels[index*self.batch_size:(index+1)*self.batch_size]\n        \n        # 만일 객체 생성 인자로 albumentation으로 만든 augmentor가 주어진다면 아래와 같이 augmentor를 이용하여 image 변환\n        # albumentations은 개별 image만 변환할 수 있으므로 batch_size만큼 할당된 image_name_batch를 한 건씩 iteration하면서 변환 수행. \n        # image_batch 배열은 float32 로 설정. \n        image_batch = np.zeros((image_name_batch.shape[0], self.image_size, self.image_size, 3), dtype='float32')\n        \n        # batch_size에 담긴 건수만큼 iteration 하면서 opencv image load -> image augmentation 변환(augmentor가 not None일 경우)-> image_batch에 담음. \n        for image_index in range(image_name_batch.shape[0]):\n            image = cv2.cvtColor(cv2.imread(image_name_batch[image_index]), cv2.COLOR_BGR2RGB)\n            if self.augmentor is not None:\n                image = self.augmentor(image=image)['image']\n            #crop 시 잘린 이미지가 원본 이미지와 다르게 되므로 augmentation 적용 후 resize() 적용. \n            image = cv2.resize(image, (self.image_size, self.image_size))\n            # 만일 preprocessing_input이 pre_func인자로 들어오면 이를 이용하여 scaling 적용. \n            if self.pre_func is not None:\n                image = self.pre_func(image)\n                \n            image_batch[image_index] = image\n        \n        return image_batch, label_batch\n    \n    # epoch가 한번 수행이 완료 될 때마다 모델의 fit()에서 호출됨. \n    def on_epoch_end(self):\n        if(self.shuffle):\n            #print('epoch end')\n            # 전체 image 파일의 위치와 label를 쌍을 맞춰서 섞어준다. scikt learn의 utils.shuffle에서 해당 기능 제공\n            self.image_filenames, self.labels = sklearn.utils.shuffle(self.image_filenames, self.labels)\n        else:\n            pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 좌우 반전(Horizontal Flip)을 수행하는 Augmentation 생성. ","metadata":{}},{"cell_type":"code","source":"import albumentations as A\n\naugmentor_light = A.Compose([\n    A.HorizontalFlip(p=0.5),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess_input\nfrom tensorflow.keras.applications.xception import preprocess_input as xcp_preprocess_input\n# 학습용, 검증용 Breed_Dataset 생성.\ntr_ds = Breed_Dataset(tr_path, tr_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                      augmentor=augmentor_light, shuffle=True, pre_func=xcp_preprocess_input)\nval_ds = Breed_Dataset(val_path, val_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                       augmentor=None, shuffle=False, pre_func=xcp_preprocess_input)\n\ntr_image_batch = next(iter(tr_ds))[0]\nval_image_batch = next(iter(val_ds))[0]\nprint(tr_image_batch.shape, val_image_batch.shape)\n\nprint(tr_image_batch[:1])\nprint(val_image_batch[:1])","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset으로 Storage에 있는 이미지 데이터를 fetch하고 가공하는데 걸리는 시간을 측정. \n* 배치 크기가 64일 경우 0.4초 정도 확보 필요. Local disk가 아닌 Object Storage일 경우 1초가 넘어서 GPU 자원 Utilization이 어려움. ","metadata":{}},{"cell_type":"code","source":"import time\n\ntr_ds = Breed_Dataset(tr_path, tr_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, augmentor=augmentor_light, shuffle=True, pre_func=xcp_preprocess_input)\nval_ds = Breed_Dataset(val_path, val_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, augmentor=None, shuffle=False, pre_func=xcp_preprocess_input)\n\n# Dataset을 64건씩 Fetch하면서 걸리는 시간 측정 \nstart = time.time()\nfor value1, value2 in iter(tr_ds):\n    end = time.time()\n    print(end - start)\n    start = end","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pretrained 모델 생성. \n* resnet50, xception, efficientnetb0, efficientnetb1 등으로 pretrained 모델을 생성할 수 있는 함수 생성. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential , Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\nfrom tensorflow.keras.applications import Xception, ResNet50V2, EfficientNetB0, EfficientNetB1\nfrom tensorflow.keras.applications import MobileNet\nimport tensorflow as tf\n\n# dog breed 종류는 120가지\n\ndef create_model(model_type='xception', in_shape=(224, 224, 3), n_classes=120):\n    input_tensor = Input(shape=in_shape)\n    if model_type == 'resnet50v2':\n        base_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'xception':\n        base_model = tf.keras.applications.Xception(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb0':\n        base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb1':\n        base_model = tf.keras.applications.EfficientNetB1(include_top=False, weights='imagenet', input_tensor=input_tensor)\n        \n    x = base_model.output  \n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)    \n    preds = Dense(units=n_classes, activation='softmax')(x)\n    model = Model(inputs=input_tensor, outputs=preds)\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 학습/검증 데이터 분할, Dataset 생성, 모델 생성, 모델 Opt, Loss설정, Learning Rate Callback 설정 함수 생성. \n* Prtrained 모델 유형, 메타 DataFrame, 초기 학습율, Augmentor, scaling 함수를 인자로 입력. \n* Learning Rate Scheduler는 ReduceLROnPlateau 적용. ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import Sequence\nimport sklearn \nimport cv2\n\nimport albumentations as A\n\nIMAGE_DIR = '/kaggle/working/Images' \n\ndef make_dogbreed_dataframe(image_dir=IMAGE_DIR):\n    paths = []\n    label_gubuns = []\n    for dirname, _, filenames in os.walk(image_dir):\n        for filename in filenames:\n            # 이미지 파일이 아닌 파일도 해당 디렉토리에 있음.\n            if '.jpg' in filename:\n                # 파일의 절대 경로를 file_path 변수에 할당. \n                file_path = dirname+'/'+ filename\n                paths.append(file_path)\n                # 이미지 파일의 절대 경로에서 레이블명 생성을 위한 1차 추출. '/'로 분할하여 파일 바로 위 서브디렉토리 이름 가져옴.  \n                start_pos = file_path.find('/', 20)\n                end_pos = file_path.rfind('/')\n                imsi_breed = file_path[start_pos+1:end_pos]\n                # 1차 추출된 데이터를 기반으로 '-' 이후 데이터가 레이블 값임. \n                breed = imsi_breed[imsi_breed.find('-')+1:]\n                #print(start_pos, end_pos, imsi_breed)\n                label_gubuns.append(breed)\n\n    data_df = pd.DataFrame({'path':paths, 'label':label_gubuns})\n    return data_df\n\n# 학습과 검증 데이터용 numpy array 분리. \ndef get_train_valid(train_df, valid_size=0.2, random_state=2021):\n    train_path = train_df['path'].values\n    train_label = pd.get_dummies(train_df['label']).values\n    \n    tr_path, val_path, tr_label, val_label = train_test_split(train_path, train_label, test_size=valid_size, random_state=random_state)\n    print('tr_path shape:', tr_path.shape, 'tr_label shape:', tr_label.shape, 'val_path shape:', val_path.shape, 'val_label shape:', val_label.shape)\n    return tr_path, val_path, tr_label, val_label\n\n\nBATCH_SIZE = 64\nIMAGE_SIZE = 224\n\n# 입력 인자 image_filenames, labels는 모두 numpy array로 들어옴. \nclass Breed_Dataset(Sequence):\n    def __init__(self, image_filenames, labels, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                 augmentor=None, shuffle=False, pre_func=None):\n        '''\n        파라미터 설명\n        image_filenames: opencv로 image를 로드할 파일의 절대 경로들\n        labels: 해당 image의 label들\n        batch_size: __getitem__(self, index) 호출 시 마다 가져올 데이터 batch 건수\n        augmentor: albumentations 객체\n        shuffle: 학습 데이터의 경우 epoch 종료시마다 데이터를 섞을지 여부\n        '''\n        # 객체 생성 인자로 들어온 값을 객체 내부 변수로 할당. \n        self.image_filenames = image_filenames\n        self.labels = labels\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.augmentor = augmentor\n        self.pre_func = pre_func\n        # train data의 경우 \n        self.shuffle = shuffle\n        if self.shuffle:\n            # 객체 생성시에 한번 데이터를 섞음. \n            #self.on_epoch_end()\n            pass\n    \n    # Sequence를 상속받은 Dataset은 batch_size 단위로 입력된 데이터를 처리함. \n    # __len__()은 전체 데이터 건수가 주어졌을 때 batch_size단위로 몇번 데이터를 반환하는지 나타남\n    def __len__(self):\n        # batch_size단위로 데이터를 몇번 가져와야하는지 계산하기 위해 전체 데이터 건수를 batch_size로 나누되, 정수로 정확히 나눠지지 않을 경우 1회를 더한다. \n        return int(np.ceil(len(self.labels) / self.batch_size))\n    \n    # batch_size 단위로 image_array, label_array 데이터를 가져와서 변환한 뒤 다시 반환함\n    # 인자로 몇번째 batch 인지를 나타내는 index를 입력하면 해당 순서에 해당하는 batch_size 만큼의 데이타를 가공하여 반환\n    # batch_size 갯수만큼 변환된 image_array와 label_array 반환. \n    def __getitem__(self, index):\n        # index는 몇번째 batch인지를 나타냄. \n        # batch_size만큼 순차적으로 데이터를 가져오려면 array에서 index*self.batch_size:(index+1)*self.batch_size 만큼의 연속 데이터를 가져오면 됨\n        image_name_batch = self.image_filenames[index*self.batch_size:(index+1)*self.batch_size]\n        if self.labels is not None:\n            label_batch = self.labels[index*self.batch_size:(index+1)*self.batch_size]\n        \n        # 만일 객체 생성 인자로 albumentation으로 만든 augmentor가 주어진다면 아래와 같이 augmentor를 이용하여 image 변환\n        # albumentations은 개별 image만 변환할 수 있으므로 batch_size만큼 할당된 image_name_batch를 한 건씩 iteration하면서 변환 수행. \n        # image_batch 배열은 float32 로 설정. \n        image_batch = np.zeros((image_name_batch.shape[0], self.image_size, self.image_size, 3), dtype='float32')\n        \n        # batch_size에 담긴 건수만큼 iteration 하면서 opencv image load -> image augmentation 변환(augmentor가 not None일 경우)-> image_batch에 담음. \n        for image_index in range(image_name_batch.shape[0]):\n            image = cv2.cvtColor(cv2.imread(image_name_batch[image_index]), cv2.COLOR_BGR2RGB)\n            if self.augmentor is not None:\n                image = self.augmentor(image=image)['image']\n            #crop 시 잘린 이미지가 원본 이미지와 다르게 되므로 augmentation 적용 후 resize() 적용. \n            image = cv2.resize(image, (self.image_size, self.image_size))\n            # 만일 preprocessing_input이 pre_func인자로 들어오면 이를 이용하여 scaling 적용. \n            if self.pre_func is not None:\n                image = self.pre_func(image)\n                \n            image_batch[image_index] = image\n        \n        return image_batch, label_batch\n    \n    # epoch가 한번 수행이 완료 될 때마다 모델의 fit()에서 호출됨. \n    def on_epoch_end(self):\n        if(self.shuffle):\n            #print('epoch end')\n            # 전체 image 파일의 위치와 label를 쌍을 맞춰서 섞어준다. scikt learn의 utils.shuffle에서 해당 기능 제공\n            self.image_filenames, self.labels = sklearn.utils.shuffle(self.image_filenames, self.labels)\n        else:\n            pass\n        \n        \naugmentor_light = A.Compose([\n    A.HorizontalFlip(p=0.5),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 30\n\ndef train_model(model_type, train_df, initial_lr=0.001, augmentor=None, input_pre_func=None):\n    tr_path, val_path, tr_label, val_label = get_train_valid(train_df, valid_size=0.2, random_state=2021)\n    \n    tr_ds = Breed_Dataset(tr_path, tr_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                          augmentor=augmentor, shuffle=True, pre_func=input_pre_func)\n    val_ds = Breed_Dataset(val_path, val_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                          augmentor=None, shuffle=False, pre_func=input_pre_func)\n\n    # model_type인자로 들어온 모델 생성. optimizer Adam적용. \n    print('#######', model_type, ' 생성 및 학습 수행 ########')\n    model = create_model(model_type=model_type)\n    model.compile(optimizer=Adam(lr=initial_lr), loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # 3번 iteration내에 validation loss가 향상되지 않으면 learning rate을 기존 learning rate * 0.2로 줄임.  \n    rlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, mode='min', verbose=1)\n    # 10번 iteration내에 validation loss가 향상되지 않으면 더 이상 학습하지 않고 종료\n    ely_cb = EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n    \n    history = model.fit(tr_ds, epochs=N_EPOCHS, steps_per_epoch=int(np.ceil(tr_path.shape[0]/BATCH_SIZE)), \n                   validation_data=val_ds, validation_steps=int(np.ceil(val_path.shape[0]/BATCH_SIZE)),\n                   callbacks=([rlr_cb, ely_cb]), verbose=1)\n    \n    return model, history\n\nIMAGE_DIR = '/kaggle/working/Images' \n\ndata_df = make_dogbreed_dataframe(image_dir=IMAGE_DIR)\ntrain_df, test_df = train_test_split(data_df, test_size=0.4, stratify=data_df['label'], random_state=2021)\nprint(train_df.shape, test_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Xception 모델로 학습/검증/테스트 성능 평가 수행. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.xception import preprocess_input as xcp_preprocess_input\n\nxception_model, xception_history = train_model(model_type='xception', train_df=train_df, initial_lr=0.0001, augmentor=augmentor_light,\n                             input_pre_func=xcp_preprocess_input)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 테스트 Dataset으로 Evaluation 및 Prediction 수행. ","metadata":{}},{"cell_type":"code","source":"test_path = test_df['path'].values\ntest_label = pd.get_dummies(test_df['label']).values\n\n# gt_class 컬럼으로 label값을 OHE에서 가장 큰 위치의 인덱스 값으로 Label encoding \ntest_df['gt_class'] = np.argmax(test_label, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 테스트 Dataset 생성 후 evaluation으로 성능 평가. \ntest_ds = Breed_Dataset(test_path, test_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                        augmentor=None, shuffle=False, pre_func=xcp_preprocess_input)\n\nxception_model.evaluate(test_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 어떤 Breed가 예측이 많이 틀렸는지 확인 \n* 실제 Ground Truth Breed와 예측 Breed가 어떻게 틀려졌는지 확인. ","metadata":{}},{"cell_type":"code","source":"# 테스트 Dataset으로 개별 image들의 predict 수행. \npredict_result = xception_model.predict(test_ds, steps=int(np.ceil(len(test_label)/BATCH_SIZE)))\npredict_class = np.argmax(predict_result, axis=1)\ntest_df['xcp_pred_class'] = predict_class","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 예측이 틀린 데이터 확인 \ntest_df[test_df['gt_class'] != test_df['xcp_pred_class']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[test_df['gt_class'] != test_df['xcp_pred_class']]['label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nplt.figure(figsize=(26, 4))\nplt.xticks(rotation=90)\n\nwrong_result_df = test_df[test_df['gt_class'] != test_df['xcp_pred_class']]\n\nsns.countplot(data=wrong_result_df, x='label')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_grid_images(image_path_list, augmentor=None, ncols=4, title=None):\n    figure, axs = plt.subplots(figsize=(22, 4), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        image = cv2.cvtColor(cv2.imread(image_path_list[i]), cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (224, 224))\n        if augmentor is not None:\n            image = augmentor(image=image)['image']\n        axs[i].imshow(image)\n        axs[i].axis('off')\n        axs[i].set_title(title) \n        \nbreed_image_list_01 = data_df[data_df['label']=='Siberian_husky']['path'].iloc[:6].tolist()\nbreed_image_list_02 = data_df[data_df['label']=='Eskimo_dog']['path'].iloc[:6].tolist()\n\nshow_grid_images(breed_image_list_01, ncols=6, title='Siberian_husky')\nshow_grid_images(breed_image_list_02, ncols=6, title='Eskimo_dog')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EfficientNetB0 모델로 학습/검증/테스트 성능 평가 수행.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\n\neffb0_model_t1, effb0_history_t1 = train_model(model_type='efficientnetb0', train_df=train_df, initial_lr=0.0001, augmentor=augmentor_light,\n                             input_pre_func=eff_preprocess_input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = test_df['path'].values\ntest_label = pd.get_dummies(test_df['label']).values\n\ntest_ds = Breed_Dataset(test_path, test_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                        augmentor=None, shuffle=False, pre_func=eff_preprocess_input)\n\neffb0_model_t1.evaluate(test_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 테스트 Dataset으로 개별 image들의 predict 수행. \npredict_result = effb0_model_t1.predict(test_ds, steps=int(np.ceil(len(test_label)/BATCH_SIZE)))\npredict_class = np.argmax(predict_result, axis=1)\ntest_df['effb0_t1_pred_class'] = predict_class","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[test_df['gt_class'] != test_df['effb0_t1_pred_class']]['label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}